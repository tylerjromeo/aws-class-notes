# S3 (Simple Storage Service)

## S3 101

among the first services made by AWS. It's a big deal. Important in all the exams.

In essentials: it stores objects of anysize that you can retrieve anywhere on the web. Any files at all.

It's *not* block baced storage

files from 0-5 TB.

Ulimited storage, you pay by the gig.

files are in "buckets" which are essentially top level folders. Buckets can have folders in it. Bucket is in a universal namespace so it has to be unique.

s3-{region}.amazonaws.com/{bucketname} is always url

upload requests give you an http 200 response

### Data consistency model

read after write consistencey for new objects

eventual consistencey for overwrites and deletes. They make take a little while to propogate (like a minute)

### s3 is key-value

an object comprises: A key (name) + data (sequence of bytes) + Version ID + metadata (upload date, tags, etc.)

subresources inside s3 include Access control lists (essentially permissions on files), torrents (beyond scope of course/exam)

### basics

built for 99.99 availability. guarantee 3 9s, and 11 9s for durability. ^very^ unlikely to lose a file in s3

tiered storage is available.

lifecycle management (move files after a certain amount of time)

versioning on files

encrypted files

access control with lists and policies that are specific to buckets.

### storage classes

s3 standard is 4 9s available, 11 9s durability, and designed to handled the loss of 2 facilities. Pretty much everyone uses this

S3 - IA is infrequently accessed, but when you need it you need it fast. Lower fee but you pay per retrieval

S3- One Zone - IA is even cheaper than IA since it's only in 1 zone.

Glacier is the cheapest, it's ussed for archiving. Expidited retrieval is expensive but you get data in like 10 minutes, standard retrieval is 3-5 hours, Bulk is like 12 hours.

THere's a table of storage tiers/classes for these different types that lays out the differences

Essentially there's a sliding scale of speed/cost standard > IA > IA 1 zone > glacier

charges are for: Storage per gig, per request, management pricing (like tags), transers, transfer accelaration (for global data transfers using cloudfront)

### exam tips

remember s3 is object based

file size is 0 to 5TB

storage is unlimited

files are store in bucket

bucket names are universally namespaced

url is s3-{region}.amazonaws.com/{bucket name}

consistency model is read after write for new objects and eventually consistent for updates and deletes

tiers are:

* standard
* IA
* one zone IA
* glacier

and know what the differences are

an s3 object has a key (name) value (data in bytes) version ID, and metadata, and subresrouces are Access control and torrent data

S3 is only for objects, not an OS or a database etc.

read s3 faqs before exam because it comes up a LOT

## Labs

### Create an s3 bucket

S3 buckets are managed at a global level. Console has one big list. Makes sense since the namespace is global

Created a `tyler-romeo-lab` bucket in US-East N. Virginia

went through properties, and added a "tag". Can also set up logging, verisoning and encryption

permissions tab lets you add accounts, public permissions and system permissions.

By default, bucket permissions are private

uploaded some files through the web console to see them in there.

If we did this through the console we'd get an http 200

went into the file uploaded and clicked "make public" so the url works

looked at permissions tab. There are 3 types of permissions: owner, other accounts (there's a list) and public. Can set read or write on all of those.

There's a properties tab where you can set the storage class (cost/availability scale), encryption, Metadata, and Tags

Metadata is headers that get set when you http at the object, which is neat.

Tags can be on individual object. Objects do not inherit the bucket's tags.

on permissions tab you can go throught he access control list, and also set up policies. There's in in browser editor and also a "policy generator" tool in the browser. policy generator is a little out of scope

management tab lets us look at lifecycle, replication, analytics, metrics, and inventory.

Notes on ecryption: can do it client side, or server side with S3 managed keys, with KMS, or with Customer Provided Keys.

### versioning

in properties tab, enable versioning.

Once versioning is enabled it can *never* be disabled, only suspended.

This can increase your storage costs if your files change very often.

uploaded a test file to play with.

made a change and uploaded again, and you can see in s3 that there are both versions.

Can download old file with a link, or could delete the new version which will restore the old version.

Could also delete the object entirely. HOWEVER, deleted files still have the old versions with a "delete" marker. You can remove this marker to restore the object.

remember that every upload makes a new copy of the file, so your storage space usage can balloon. Each version is its own object.

s3 versioning docs are useful to read through to get into the technical details.

An important thing on these docs is MFA delete. That means you can add a hardware token requirement for deleting objects or changing the version control strategy.

#### exam tips

versioning stores every version of an object

it's good for backups

once it's enabled it can't be turned off, only suspended

integrates with lifecycle tools (we'll get into this later)

Has MFA delete option for further protecting your backups

### cross-region replication

created a new bucket in the us west bucket

have to enable versioning for cross region replication to work

can replicate an entire bucket or just a sub folder

pick a destination bucket for replication. Can change the storage class for destinaion (IA is good here probably)

need an IAM role for doing the replication (it can create a new one for you)

replication is only for NEW or CHANGED object, it does not do existing objects

to copy over the contents of an existing bucket, you can use the cli

#### sidebar, setting up the aws cli

already had this installed

making a new IAM group for CLI access

once cli was set up with a new user, ran: `aws s3 ls` to see all the buckets

`aws s3 cp --recursive s3://tyler-romeo-lab s://tyler-romeo-westcoast` to copy files from one bucket to the other

after copying over, deleted a file from main bucket. That delete marker gets replicated over

however, when you delete the delete marker, it does NOT get replicated. Just something to know

updated a file in the main bucket, that update gets replicated to the other region

interesting note the old versions of my objects in the original bucklet did not get `cp`d over.

another weird thing, if you revert to an old version by deleting from a source bucket, that revert doesn't get replcated. Have to do that manually.

Looks like cross region updates are best when there's not a lot of deleting going on in the source bucket

#### Exam tips

both source and destination must have versioning turned on

can't replicate into the same bucket with this tool

files in existing bucket don't get replicated, it just applies updates to both buckets.

Can't replicated to multiple destinations, and can't daisy chain from bucket to bucket (right now)

delete markers are replicated (since they are objects) but *deletions* of objects are not replicated. It's a weird distinction

Understand the high level implications of cross region replication.

### Lifecycle Management, S3-IA & Glacier

Use case includes needing to store files for a long time, but not get to them very often

Made a new s3 bucket with versioning turned on

went into settings under management tab to look at lifecycle rules

lifecycle rules define how s3 manages objects, it's a way to automatically manage tiered storage, auto-expire objects, etc.

can apply lifecycle rules to specific filters/tags if you want

added rule to transition objects to standard-IA after 30 days, and another for moving to Glacier after 60 days.

"previous versions" can have their own separate rules. That's how it deals with old versions of objects when versinoing is turned on

set up rules to "expire" objects after 425 days

Not super complex, but important

#### Exam tips

Can be used with versioning

Can be used with bot cuyrrent and previous versions

Can transition to standard IA, can archive to Glacier, or can permenantly deleted after set intervals

Will probably get csenario questions about minimizing storage costs, this is how you would accomplish that.